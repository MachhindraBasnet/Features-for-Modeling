{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Introduction**\n\nIntracranial hemorrhage, bleeding that occurs inside the cranium, is a serious health problem that requires rapid and often intensive medical treatment. Intracranial hemorrhages account for approximately 10% of strokes in the U.S., where stroke is the fifth-leading cause of death. Identifying the location and type of any hemorrhage present is a critical step in treating the patient.\nDiagnosis requires an urgent procedure. When a patient shows acute neurological symptoms such as severe headache or loss of consciousness, highly trained specialists review medical images of the patientâ€™s cranium to look for the presence, location and type of hemorrhage. The process is complicated and often time consuming.\n\nFor this caspstone, I am using using RSNA intracranial hemorrhage detection data set from Kaggle, (https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/overview/description) in order to identify acute intracranial hemorrhage and its subtypes from the CT study images of patients. The CT images can confirm bleeding and the evidence of trauma in head. Correct diagnosis of presence of hemorrhage and its type looking at the radiological report of the patient helps timely and effective care. The dataset consists of CT studies of patients and the probability of whether the type of hemorrhage exists or not. \n\n\nMy objective is to build an algorithm that can detect hemorrhage and its type which could be a valuable information for the medical community to make data driven decisions. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the required libraries\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\nimport pydicom\n\nfrom os import listdir\n\nfrom skimage.transform import resize\nfrom imgaug import augmenters as iaa\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.applications import ResNet50\nimport cv2\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint \nfrom keras.layers import LSTM, Input, TimeDistributed\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop, SGD\nfrom keras import layers\nfrom keras.applications import DenseNet121\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom tqdm import tqdm\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"listdir(\"../input/rsna-intracranial-hemorrhage-detection/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_PATH = \"../input/rsna-intracranial-hemorrhage-detection/\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"**Importing CSVs and Images**\n\nThe data consists of CT images as stage-1_train.zpi and stage-1_test.zip and data  for train and test. The dataset also consists of IDs of the patients as (stage_1_train.csv ) (stage_1_sample_submission.csv) and multiple labels, one for each of five sub-types of hemorrhage, plus an additional label for any, which should always be true if any of the sub-type labels is true.\n\nI used a Kaggle kernel to avoid  donwloading this big dataset set to my local machine. Further I am using only the stage_1_train images and stage_1_train.csv  data which includes patient information of the respective images. I have splitted the training data further into train and evaluation sets.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf = pd.read_csv(INPUT_PATH + \"stage_1_train.csv\")\ntraindf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf['type'] = traindf['ID'].str.split(\"_\", n = 3, expand = True)[2]\ntraindf['PatientID'] = traindf['ID'].str.split(\"_\", n = 3, expand = True)[1]\ntraindf['filename'] = traindf['ID'].apply(lambda st: \"ID_\" + st.split('_')[1] + \".png\")\ntraindf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf.filename.nunique()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see if the filename matches the number of images"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_directory = INPUT_PATH + \"stage_1_train_images/\"\ntrain_files = listdir(train_directory)\n\ntrain_size = len(train_files)\ntrain_size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the number of filenames matches the number of images in the image directory"},{"metadata":{},"cell_type":"markdown","source":"**Data Visualization**\n\nLet us plot a digram to visualize the distribution of label in the dataset and also see the distribution by type of hemorrhage"},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf['Label'].value_counts()\nsns.countplot(x= 'Label', data = traindf);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the data imbalance here (The images with hemorrhage being less). We will try balancing before building our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf['Label'].value_counts()\nsns.countplot(x= 'Label', data = traindf, hue = 'type');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing the images**\n\nRescaling resizing and converting the .dcm imgaes to .png"},{"metadata":{"trusted":true},"cell_type":"code","source":"def window_image(img, window_center,window_width, intercept, slope, rescale=True):\n\n    img = (img*slope +intercept)\n    img_min = window_center - window_width//2\n    img_max = window_center + window_width//2\n    img[img<img_min] = img_min\n    img[img>img_max] = img_max\n    \n    if rescale:\n        # Extra rescaling to 0-1, not in the original notebook\n        img = (img - img_min) / (img_max - img_min)\n    \n    return img\n    \ndef get_first_of_dicom_field_as_int(x):\n    #get x[0] as in int is x is a 'pydicom.multival.MultiValue', otherwise get int(x)\n    if type(x) == pydicom.multival.MultiValue:\n        return int(x[0])\n    else:\n        return int(x)\n\ndef get_windowing(data):\n    dicom_fields = [data[('0028','1050')].value, #window center\n                    data[('0028','1051')].value, #window width\n                    data[('0028','1052')].value, #intercept\n                    data[('0028','1053')].value] #slope\n    return [get_first_of_dicom_field_as_int(x) for x in dicom_fields]\n\n\ndef view_images(images, title = '', aug = None):\n    width = 5\n    height = 2\n    fig, axs = plt.subplots(height, width, figsize=(15,5))\n    \n    for im in range(0, height * width):\n        data = pydicom.read_file(os.path.join(INPUT_PATH + \"stage_1_train_images/\",'ID_'+images[im]+ '.dcm'))\n        image = data.pixel_array\n        window_center , window_width, intercept, slope = get_windowing(data)\n        image_windowed = window_image(image, window_center, window_width, intercept, slope)\n\n\n        i = im // width\n        j = im % width\n        axs[i,j].imshow(image_windowed, cmap=plt.cm.bone) \n        axs[i,j].axis('off')\n        \n    plt.suptitle(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_and_resize(filenames, load_dir):   \n    X=[] \n    \n    save_dir = '/kaggle/tmp/'\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    for filename in tqdm(filenames):\n        path = load_dir + filename\n        new_path = save_dir + filename.replace( '.dcm', '.png' )\n        \n        dcm = pydicom.dcmread(path)\n        window_center , window_width, intercept, slope = get_windowing(dcm)\n        img = dcm.pixel_array\n        img = window_image(img, window_center, window_width, intercept, slope)\n        \n        resized = cv2.resize(img, (224, 224)) \n        res = cv2.imwrite(new_path, resized)\n        if not res:\n            print('Failed')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualization of images**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_files[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets see what the images contain\ndataset = pydicom.dcmread(train_directory + \"ID_3b59681d3.dcm\")\nprint(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subtypes = traindf.type.unique()\nsubtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"view_images(traindf[(traindf['type'] == 'epidural') & (traindf['Label'] == 1)][:10].PatientID.values, title = 'Images of Epidural Hemorrhage')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"view_images(traindf[(traindf['type'] == 'intraparenchymal') & (traindf['Label'] == 1)][:10].PatientID.values, title = 'Images of Intraparenchymal Hemorrhage')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"view_images(traindf[(traindf['type'] == 'intraventricular') & (traindf['Label'] == 1)][:10].PatientID.values, title = 'Images of Intraventricular Hemorrhage')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"view_images(traindf[(traindf['type'] == 'subarachnoid') & (traindf['Label'] == 1)][:10].PatientID.values, title = 'Images of Subarachnoid Hemorrhage')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"view_images(traindf[(traindf['type'] == 'subdural') & (traindf['Label'] == 1)][:10].PatientID.values, title = 'Images of Subdural Hemorrhage')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preparing the dataframe\n**\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are taking random sample of 1000 images. This is only for faster run time, with more CPU, GPU we can use all images. "},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\nsample_data = np.random.choice((train_files), 1000)  \nsample_df = traindf[traindf.filename.apply(lambda x: x.replace('.png', '.dcm')).isin(sample_data)]\n\npivot_df = sample_df[['Label', 'filename', 'type']].drop_duplicates().pivot(\n    index='filename', columns='type', values='Label').reset_index()\nprint(pivot_df.shape)\npivot_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nsave_and_resize(filenames=sample_data, load_dir=INPUT_PATH + \"stage_1_train_images/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading each of the 1000 images with opencv and saving as matrix "},{"metadata":{"trusted":true},"cell_type":"code","source":"X=[]\ny=[] \npath =  '/kaggle/tmp/' \nimport cv2  \nimport glob \n\nfor i, row in pivot_df.iterrows():  \n    file = row['filename'] \n    if path+ file in glob.glob('/kaggle/tmp/*.png') : \n        img=path+file\n        img=cv2.imread(img, 0) #grey \n        #print(img.shape)\n        X.append(img)\n        y.append(row['any'])  \n    else: \n        continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X= np.array(X)\ny= np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(y);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the data is imbalanced. We will balance the data by undersampling the majority class (this is to save the omputational time as well due to the limitation of CPU/GPU)"},{"metadata":{},"cell_type":"markdown","source":"Let us split the data to train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Balancing the data by undersample the majority class"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Oversample\n#from imblearn.over_sampling import SMOTE\n#Let us upsample the dataset\n#oversampler= SMOTE(random_state=0)\n#X_train_final, y_train_final = oversampler.fit_sample(X_train.reshape(X_train.shape[0], -1), y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(X_train_final.shape)\n# print(y_train_final.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Undersample\nfrom imblearn.under_sampling import RandomUnderSampler\nrus = RandomUnderSampler(random_state=0)\nX_train_final, y_train_final = rus.fit_resample(X_train.reshape(X_train.shape[0], -1), y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_final.shape, y_train_final.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalizing the pixel values between 0 and 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train= X_train_final.reshape(X_train_final.shape[0], 224, 224)/255\ny_train = y_train_final \nX_test = X_test/255\ny_test = y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train= X_train/255\ny_train = y_train\nX_test = X_test/255\ny_test = y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Reshaping the images to 3D for fitting  NN models"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_rows, img_cols = 224, 224\nX_train = X_train.reshape((-1, img_rows, img_cols, 1))\ny_train = y_train\nX_test = X_test.reshape((-1, img_rows, img_cols, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(y_train);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the balanced train data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(X_train[1].reshape(224, 224)) \nprint(y_train[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Building Models**"},{"metadata":{},"cell_type":"markdown","source":"Dense model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Add\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint \n\n\n \n    \n# class_weight = {0: 10,\n#                 1: 1 } \n\nmodel_dense = Sequential()\n\n# Add dense layers to create a fully connected MLP\n# Note that we specify an input shape for the first layer, but only the first layer.\n# Relu is the activation function used\nmodel_dense.add(Dense(32, activation='relu', input_shape=(224, 224,1)))\n# Dropout layers remove features and fight overfitting\nmodel_dense.add(Dropout(0.4))\nmodel_dense.add(Dense(32, activation='relu'))\nmodel_dense.add(Dropout(0.4))\n# End with a number of units equal to the number of classes we have for our outcome\nmodel_dense.add(Flatten())\nmodel_dense.add(Dense(1, activation='softmax'))\n\n\nmodel_dense.summary()\n\n\n# Compile the model to put it all together.\nmodel_dense.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nhistory_dense = model_dense.fit(X_train, y_train,\n                          batch_size=64,\n                          epochs=3,\n                          verbose=1,\n                          validation_data=(X_test, y_test))\nscore = model_dense.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= model_dense.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Evaluation\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convolutional Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint \n\n\nbatch_size=64\nvalidation_ratio=0.1\nsample_size=200\nepochs=3 \n\nmodel = Sequential()\n\nmodel.add(Conv2D(64, (3, 3), input_shape=(224, 224,1)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64,(3,3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Flatten())  \n\nmodel.add(Dense(100))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(50))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(1))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel.fit(X_train,y_train,batch_size=batch_size,epochs=3,validation_data=(X_test, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Evaluation\nprint(confusion_matrix(y_test, y_pred))  \nprint(classification_report(y_test, y_pred)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ResNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshape channel to 3 for resnet\n# One hot on y \nfrom keras.utils.np_utils import to_categorical\nX_train_rgb = np.repeat(X_train, 3, -1)\nX_test_rgb = np.repeat(X_test, 3, -1)\ny_train_rgb= to_categorical(y_train)\ny_test_rgb= to_categorical(y_test)\nX_train_rgb.shape , X_test_rgb.shape, y_train_rgb.shape, y_test_rgb.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.resnet50 import ResNet50 \nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D, Input, BatchNormalization, Dropout , Concatenate\nfrom keras import backend as K\n\n        \nbase_model = ResNet50(weights='imagenet',include_top=False, input_shape=(224, 224, 3))\nlast_layer = base_model.output\nx = GlobalAveragePooling2D()(last_layer)\nx = Dense(6, activation='relu',name='fc-1')(x)\nx = Dropout(0.5)(x)\nout = Dense(2, activation='softmax',name='output_layer')(x)\n\n\nresnet_model = Model(inputs=base_model.input, outputs=out)\nresnet_model.summary()\n\nfor layer in resnet_model.layers[:-4]:\n    layer.trainable = False\n\nresnet_model.layers[-1].trainable\nresnet_model.compile(loss='categorical_crossentropy',\n                                 optimizer='adam',metrics=['accuracy'])\nresnet_model.fit(X_train_rgb, y_train_rgb, batch_size=32, epochs=3, verbose=1, validation_data=(X_test_rgb, y_test_rgb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = resnet_model.predict(X_test_rgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=np.argmax(y_pred, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Evaluation\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**VGG Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\n\nbase_model = VGG16(weights='imagenet',include_top=False, input_shape=(224, 224, 3))\nlast_layer = base_model.output\nx = GlobalAveragePooling2D()(last_layer)\nx = Dense(6, activation='relu',name='fc-1')(x)\nx = Dropout(0.5)(x)\nout = Dense(2, activation='softmax',name='output_layer')(x)\n\n\nvgg_model = Model(inputs=base_model.input, outputs=out)\nvgg_model.summary()\n\n# for layer in resnet_model.layers[:-4]:\n#     layer.trainable = False\n\nvgg_model.layers[-1].trainable\nvgg_model.compile(loss='categorical_crossentropy',\n                                 optimizer='adam',metrics=['accuracy'])\nvgg_model.fit(X_train_rgb, y_train_rgb, batch_size=32, epochs=5, verbose=1, validation_data=(X_test_rgb, y_test_rgb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = vgg_model.predict(X_test_rgb)\ny_pred=np.argmax(y_pred, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusions\n\nWe have created workable pipelines to preprocess dcm data and fit into severeal CNNs \n\nThe models haven't led to useable results due to the limitation on computational power. \n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"Next steps\n\nTrain models on AWS/google cloud using more images (with augumentation) as input \n\nTry fastai and other pretrained models\n\nCompare model performance\n\nCreate front end tool/app for users to interact with the model results \n\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}